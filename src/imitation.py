import sys
import argparse
import numpy as np
import keras
import random
import gym

from keras import metrics
import tensorflow as tf
from tensorflow import set_random_seed
from keras.optimizers import Adam

from keras.utils.np_utils import to_categorical
from keras.optimizers import Adam


class Imitation():
    def __init__(self, model_config_path, expert_weights_path):
        # Load the expert model.
        with open(model_config_path, 'r') as f:
            self.expert = keras.models.model_from_json(f.read())
        self.expert.load_weights(expert_weights_path)
        
        # Initialize the cloned model (to be trained).
        with open(model_config_path, 'r') as f:
            self.model = keras.models.model_from_json(f.read())

        # TODO: Define any training operations and optimizers here, initialize
        #       your variables, or alternatively compile your model here.
        self.optimizer = Adam(lr=0.003)
        self.model.compile(loss='categorical_crossentropy',\
                           optimizer=self.optimizer,\
                           metrics=[metrics.categorical_crossentropy, 'acc'])

        # Setting the batch size
        self.batch_size = 32

        print('Finished initializing')

    def run_expert(self, env, render=False):
        # Generates an episode by running the expert policy on the given env.
        return Imitation.generate_episode(self.expert, env, render)

    def run_model(self, env, render=False):
        # Generates an episode by running the cloned policy on the given env.
        return Imitation.generate_episode(self.model, env, render)

    @staticmethod
    def generate_episode(model, env, render=False):
        # Generates an episode by running the given model on the given env.
        # Returns:
        # - a list of states, indexed by time step
        # - a list of actions, indexed by time step
        # - a list of rewards, indexed by time step
        # TODO: Implement this method.
        states = []
        actions = []
        rewards = []

        s = env.reset()
        s = np.array(s)
        done = False
        while(done != True):
            
            s = np.reshape(s,[1,8])
            action_softmax = model.predict(s)
            action = np.argmax(action_softmax)
            action_1hot = to_categorical(action, num_classes=4)
            nexts, reward, done, _ = env.step(action)
            nexts = np.array(nexts)
            
            # Append the s,a,r for the current time-step
            states.append(s)
            actions.append(action_1hot)
            rewards.append(reward)

            s = nexts   

        return states, actions, rewards

    def test_expert(self, env, num_episodes=50, render=False):
        cum_reward_list = []

        for n_episode in range(num_episodes):
            cum_reward = 0
            s = env.reset()
            s = np.array(s)
            done = False
            while (done != True):
                s = np.reshape(s, [1,8])
                action_softmax = self.expert.predict(s)
                action = np.argmax(action_softmax)
                nexts, reward, done, _ = env.step(action)
                nexts = np.array(nexts)
                s = nexts

                cum_reward += reward

            cum_reward_list.append(cum_reward)

        print("Cum. Reward Mean:{}, Std:{}".format(np.mean(cum_reward_list), np.std(cum_reward_list)))
        return cum_reward_list

    def test(self, env, num_episodes=50, render=False):
        cum_reward_list = []

        for n_episode in range(num_episodes):
            cum_reward = 0
            s = env.reset()
            s = np.array(s)
            done = False
            while (done != True):
                s = np.reshape(s, [1,8])
                action_softmax = self.model.predict(s)
                action = np.argmax(action_softmax)
                nexts, reward, done, _ = env.step(action)
                nexts = np.array(nexts)
                s = nexts

                cum_reward += reward

            cum_reward_list.append(cum_reward)

        print("Cum. Reward Mean:{}, Std:{}".format(np.mean(cum_reward_list), np.std(cum_reward_list)))
        return cum_reward_list

    
    def train(self, env, num_episodes=100, num_epochs=50, render=False):
        # Trains the model on training data generated by the expert policy.
        # Args:
        # - env: The environment to run the expert policy on. 
        # - num_episodes: # episodes to be generated by the expert.
        # - num_epochs: # epochs to train on the data generated by the expert.
        # - render: Whether to render the environment.
        # Returns the final loss and accuracy.
        # TODO: Implement this method. It may be helpful to call the class
        #       method run_expert() to generate training data.
        loss = 0
        acc = 0

        # Prepare the input data and label
        x_train = []
        y_train = []


        for i in range(num_episodes):
            states, actions, rewards = self.run_expert(env, render)
            x_train.extend(states)
            y_train.extend(actions)

        x_train = np.array(x_train)
        x_train = np.reshape(x_train, [x_train.shape[0], x_train.shape[2]])
        y_train = np.array(y_train)


        # Training the model
        history = self.model.fit(x_train, y_train, \
                       batch_size=self.batch_size, \
                       epochs=num_epochs, \
                       verbose=1)

        # Get the loss and accuracy from history
        acc = history.history['acc'][49]

        return loss, acc


def parse_arguments():
    # Command-line flags are defined here.
    parser = argparse.ArgumentParser()
    parser.add_argument('--model-config-path', dest='model_config_path',
                        type=str, default='LunarLander-v2-config.json',
                        help="Path to the model config file.")
    parser.add_argument('--expert-weights-path', dest='expert_weights_path',
                        type=str, default='LunarLander-v2-weights.h5',
                        help="Path to the expert weights file.")

    # https://stackoverflow.com/questions/15008758/parsing-boolean-values-with-argparse
    parser_group = parser.add_mutually_exclusive_group(required=False)
    parser_group.add_argument('--render', dest='render',
                              action='store_true',
                              help="Whether to render the environment.")
    parser_group.add_argument('--no-render', dest='render',
                              action='store_false',
                              help="Whether to render the environment.")
    parser.set_defaults(render=False)

    return parser.parse_args()


def main(args):
    # Parse command-line arguments.
    args = parse_arguments()
    model_config_path = args.model_config_path
    expert_weights_path = args.expert_weights_path
    render = args.render

    # Setting the session to allow growth, so it doesn't allocate all GPU memory.
    gpu_ops = tf.GPUOptions(allow_growth=True)
    config = tf.ConfigProto(gpu_options=gpu_ops)
    sess = tf.Session(config=config)

    # Setting this as the default tensorflow session.
    keras.backend.tensorflow_backend.set_session(sess)
    
    # Create the environment.
    env = gym.make('LunarLander-v2')

    # Set the random seeds
    env.seed(2018)
    np.random.seed(2018)
    set_random_seed(2018)
    
    # TODO: Train cloned models using imitation learning, and record their
    #       performance.

    # Training the model by collecting data over 1 episode
    imitation_1 = Imitation(model_config_path, expert_weights_path)
    _, acc1 = imitation_1.train(env, 1, 50, True)

    # Training the model by collecting data over 10 episode
    imitation_10 = Imitation(model_config_path, expert_weights_path)
    _, acc10 = imitation_10.train(env, 10, 50, True)

    # Training the model by collecting data over 50 episode
    imitation_50 = Imitation(model_config_path, expert_weights_path)
    _, acc50 = imitation_50.train(env, 50, 50, True)

    # Training the model by collecting data over 100 episode
    imitation_100 = Imitation(model_config_path, expert_weights_path)
    _, acc100 = imitation_100.train(env, 100, 50, True)

    # Train accuracies
    print("Train accuracies..................")
    print("Accuracy of model trained on 1 episode of expert data: {}".format(acc1))
    print("Accuracy of model trained on 10 episode of expert data: {}".format(acc10))
    print("Accuracy of model trained on 50 episode of expert data: {}".format(acc50))
    print("Accuracy of model trained on 100 episode of expert data: {}".format(acc100))

    # Testing
    print("Testing.......................")
    print("Expert test results: ")
    imitation_1.test_expert(env, num_episodes=50)
    print("Testing model trained on 1 episode of expert data: ")
    imitation_1.test(env, num_episodes=50)
    print("Testing model trained on 10 episode of expert data: ")
    imitation_10.test(env, num_episodes=50)
    print("Testing model trained on 50 episode of expert data: ")
    imitation_50.test(env, num_episodes=50)
    print("Testing model trained on 100 episode of expert data: ")
    imitation_100.test(env, num_episodes=50)


if __name__ == '__main__':
  main(sys.argv)
